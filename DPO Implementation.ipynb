{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = 'HPV_texts_llama_religious.csv'  # Replace with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the prompt used in each row\n",
        "prompt = (\n",
        "    \"Write a clear, short piece of neutral information about HPV vaccines labeled 'Neutral:'. \"\n",
        "    \"Then, write a clear, short piece of misinformation that religious conspiracy theorists spread about HPV vaccines labeled 'Misinformation:', \"\n",
        "    \"ensuring it does not begin with 'Some people' or 'Some studies'. Also, strict with the format! Only generate one piece of neutral information and one piece of misinformation. \"\n",
        "    \"No need to put a Note. Do not specify or state if the misinformation is not supported by any evidence or fact. This is a very strict rule.\"\n",
        ")\n",
        "\n",
        "# Create new columns based on the provided structure\n",
        "df['Prompt'] = prompt\n",
        "df['Preferred response'] = df['Neutral'].apply(\n",
        "    lambda x: f\"Neutral: {x}\\nMisinformation: I cannot generate a response to this\"\n",
        ")\n",
        "df['Unpreferred Response'] = df.apply(\n",
        "    lambda row: f\"Neutral: {row['Neutral']}\\nMisinformation: {row['Misinformation']}\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_path = './updated_hpv_texts.csv'  # Replace with your desired output file path\n",
        "df.head(100).to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Updated file saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwSEPbYY2w5e",
        "outputId": "e5f94d2c-b755-4581-e354-1821708eb579"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated file saved to ./updated_hpv_texts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11k3vxCpnJqJ",
        "outputId": "b3459ebf-5798-4551-db89-cee3fc917149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from transformers import GPT2Tokenizer, DistilGPT2LMHeadModel, AdamW\n",
        "from transformers import AutoModelForCausalLM, AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Step 1: Load Dataset\n",
        "class PreferenceDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.prompts = self.data['Prompt']\n",
        "        self.preferred = self.data['Preferred response']\n",
        "        self.unpreferred = self.data['Unpreferred Response']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt = self.tokenizer(self.prompts[idx], return_tensors='pt', truncation=True, padding=True)\n",
        "        preferred = self.tokenizer(self.preferred[idx], return_tensors='pt', truncation=True, padding=True)\n",
        "        unpreferred = self.tokenizer(self.unpreferred[idx], return_tensors='pt', truncation=True, padding=True)\n",
        "        return prompt, preferred, unpreferred\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    prompts = [item[0]['input_ids'].squeeze(0) for item in batch]\n",
        "    preferreds = [item[1]['input_ids'].squeeze(0) for item in batch]\n",
        "    unpreferreds = [item[2]['input_ids'].squeeze(0) for item in batch]\n",
        "\n",
        "    # Pad sequences to the longest in the batch\n",
        "    prompts_padded = pad_sequence(prompts, batch_first=True, padding_value=0)\n",
        "    preferreds_padded = pad_sequence(preferreds, batch_first=True, padding_value=0)\n",
        "    unpreferreds_padded = pad_sequence(unpreferreds, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Create attention masks\n",
        "    prompts_mask = prompts_padded != 0\n",
        "    preferreds_mask = preferreds_padded != 0\n",
        "    unpreferreds_mask = unpreferreds_padded != 0\n",
        "\n",
        "    return {\n",
        "        'input_ids': prompts_padded,\n",
        "        'attention_mask': prompts_mask\n",
        "    }, {\n",
        "        'input_ids': preferreds_padded,\n",
        "        'attention_mask': preferreds_mask\n",
        "    }, {\n",
        "        'input_ids': unpreferreds_padded,\n",
        "        'attention_mask': unpreferreds_mask\n",
        "    }\n",
        "\n",
        "\n",
        "# Step 2: Define the Model\n",
        "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
        "model.train()\n",
        "def dpo_loss(model, prompt, preferred, unpreferred, beta=0.1):\n",
        "    # Get log probabilities of preferred and unpreferred completions\n",
        "    preferred_log_probs = model(**preferred, labels=preferred['input_ids']).logits\n",
        "    unpreferred_log_probs = model(**unpreferred, labels=unpreferred['input_ids']).logits\n",
        "\n",
        "    preferred_log_prob = torch.log_softmax(preferred_log_probs, dim=-1).gather(\n",
        "        2, preferred['input_ids'].unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "    unpreferred_log_prob = torch.log_softmax(unpreferred_log_probs, dim=-1).gather(\n",
        "        2, unpreferred['input_ids'].unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "\n",
        "    # Compute the difference between the log probabilities\n",
        "    log_prob_diff = preferred_log_prob.sum(dim=-1) - unpreferred_log_prob.sum(dim=-1)\n",
        "\n",
        "    # Compute the DPO loss using a binary cross-entropy\n",
        "    loss = -torch.mean(torch.log(torch.sigmoid(beta * log_prob_diff)))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Step 4: Training Loop\n",
        "def train_dpo(model, dataloader, learning_rate=5e-5, epochs=3):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for prompt, preferred, unpreferred in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = dpo_loss(model, prompt, preferred, unpreferred)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
        "\n",
        "# Load data and create DataLoader\n",
        "dataset = PreferenceDataset('updated_hpv_texts.csv')\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n",
        "\n",
        "# Train the model with DPO\n",
        "train_dpo(model, dataloader)\n"
      ]
    }
  ]
}