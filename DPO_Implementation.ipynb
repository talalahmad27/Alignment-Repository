{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = 'HPV_texts_llama_religious.csv'  # Replace with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the prompt used in each row\n",
        "prompt = (\n",
        "    \"Write a clear, short piece of neutral information about HPV vaccines labeled 'Neutral:'. \"\n",
        "    \"Then, write a clear, short piece of misinformation that religious conspiracy theorists spread about HPV vaccines labeled 'Misinformation:', \"\n",
        "    \"ensuring it does not begin with 'Some people' or 'Some studies'. Also, strict with the format! Only generate one piece of neutral information and one piece of misinformation. \"\n",
        "    \"No need to put a Note. Do not specify or state if the misinformation is not supported by any evidence or fact. This is a very strict rule.\"\n",
        ")\n",
        "\n",
        "# Create new columns based on the provided structure\n",
        "df['Prompt'] = prompt\n",
        "df['Preferred response'] = df['Neutral'].apply(\n",
        "    lambda x: f\"Neutral: {x}\\nMisinformation: I cannot generate a response to this\"\n",
        ")\n",
        "df['Unpreferred Response'] = df.apply(\n",
        "    lambda row: f\"Neutral: {row['Neutral']}\\nMisinformation: {row['Misinformation']}\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_path = './updated_hpv_texts.csv'  # Replace with your desired output file path\n",
        "df.head(100).to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Updated file saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwSEPbYY2w5e",
        "outputId": "e5f94d2c-b755-4581-e354-1821708eb579"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated file saved to ./updated_hpv_texts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11k3vxCpnJqJ",
        "outputId": "b1b5646f-3a89-4c01-e9fe-642cf9b0ec39"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.0\n",
            "Epoch 2, Loss: 0.0\n",
            "Epoch 3, Loss: 0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from transformers import GPT2Tokenizer, DistilGPT2LMHeadModel, AdamW\n",
        "from transformers import AutoModelForCausalLM, AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Step 1: Load Dataset\n",
        "class PreferenceDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.prompts = self.data['Prompt']\n",
        "        self.preferred = self.data['Preferred response']\n",
        "        self.unpreferred = self.data['Unpreferred Response']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt = self.tokenizer(self.prompts[idx], return_tensors='pt', truncation=True, padding=True)\n",
        "        preferred = self.tokenizer(self.preferred[idx], return_tensors='pt', truncation=True, padding=True)\n",
        "        unpreferred = self.tokenizer(self.unpreferred[idx], return_tensors='pt', truncation=True, padding=True)\n",
        "        return prompt, preferred, unpreferred\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    prompts = [item[0]['input_ids'].squeeze(0) for item in batch]\n",
        "    preferreds = [item[1]['input_ids'].squeeze(0) for item in batch]\n",
        "    unpreferreds = [item[2]['input_ids'].squeeze(0) for item in batch]\n",
        "\n",
        "    # Pad sequences to the longest in the batch\n",
        "    prompts_padded = pad_sequence(prompts, batch_first=True, padding_value=0)\n",
        "    preferreds_padded = pad_sequence(preferreds, batch_first=True, padding_value=0)\n",
        "    unpreferreds_padded = pad_sequence(unpreferreds, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Create attention masks\n",
        "    prompts_mask = prompts_padded != 0\n",
        "    preferreds_mask = preferreds_padded != 0\n",
        "    unpreferreds_mask = unpreferreds_padded != 0\n",
        "\n",
        "    return {\n",
        "        'input_ids': prompts_padded,\n",
        "        'attention_mask': prompts_mask\n",
        "    }, {\n",
        "        'input_ids': preferreds_padded,\n",
        "        'attention_mask': preferreds_mask\n",
        "    }, {\n",
        "        'input_ids': unpreferreds_padded,\n",
        "        'attention_mask': unpreferreds_mask\n",
        "    }\n",
        "\n",
        "\n",
        "# Step 2: Define the Model\n",
        "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
        "model.train()\n",
        "def dpo_loss(model, prompt, preferred, unpreferred, beta=0.1):\n",
        "    # Get log probabilities of preferred and unpreferred completions\n",
        "    preferred_log_probs = model(**preferred, labels=preferred['input_ids']).logits\n",
        "    unpreferred_log_probs = model(**unpreferred, labels=unpreferred['input_ids']).logits\n",
        "\n",
        "    preferred_log_prob = torch.log_softmax(preferred_log_probs, dim=-1).gather(\n",
        "        2, preferred['input_ids'].unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "    unpreferred_log_prob = torch.log_softmax(unpreferred_log_probs, dim=-1).gather(\n",
        "        2, unpreferred['input_ids'].unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "\n",
        "    # Compute the difference between the log probabilities\n",
        "    log_prob_diff = preferred_log_prob.sum(dim=-1) - unpreferred_log_prob.sum(dim=-1)\n",
        "\n",
        "    # Compute the DPO loss using a binary cross-entropy\n",
        "    loss = -torch.mean(torch.log(torch.sigmoid(beta * log_prob_diff)))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Step 4: Training Loop\n",
        "def train_dpo(model, dataloader, learning_rate=5e-5, epochs=3):\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for prompt, preferred, unpreferred in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            loss = dpo_loss(model, prompt, preferred, unpreferred)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
        "\n",
        "# Load data and create DataLoader\n",
        "dataset = PreferenceDataset('updated_hpv_texts.csv')\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n",
        "\n",
        "# Train the model with DPO\n",
        "train_dpo(model, dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = './dpo_trained'\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Step 6: Load the Model for Generation (after training)\n",
        "model = AutoModelForCausalLM.from_pretrained(save_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "model.eval()  # Set the model to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CM2CXe9AXxj",
        "outputId": "17249011-72ea-431d-e253-a1753979eac1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Step 7: Generate a Response\n",
        "def generate_response(model, tokenizer, prompt, max_length=150, temperature=0.7, num_return_sequences=1):\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            do_sample=True,  # Enable sampling to introduce randomness\n",
        "            top_k=50,        # Use top-k sampling for more varied responses\n",
        "            top_p=0.95       # Use nucleus sampling for diverse generations\n",
        "        )\n",
        "    responses = [tokenizer.decode(output_id, skip_special_tokens=True) for output_id in output_ids]\n",
        "    return responses[0] if num_return_sequences == 1 else responses\n",
        "\n",
        "# Example Usage\n",
        "#prompt = \"Write a clear, short piece of neutral information about HPV vaccines labeled 'Neutral:'. Then, write a clear, short piece of misinformation that religious conspiracy theorists spread about HPV vaccines labeled 'Misinformation:', ensuring it does not begin with 'Some people' or 'Some studies'. Also, strict with the format! Only generate one piece of neutral information and one piece of misinformation. No need to put a Note. Do not specify or state if the misinformation is not supported by any evidence or fact. This is a very strict rule.\"\n",
        "prompt = \"Hi How are you \"\n",
        "response = generate_response(model, tokenizer, prompt)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wqlcDtoA1xk",
        "outputId": "764bfc70-7617-4265-adbd-71af532a29fd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Hi How are you \n",
            "Generated Response: Hi How are you!!! I have been working with a lot of the people who worked on the game, and what are your thoughts? What are your thoughts? What are your thoughts? I am always looking for people who have been on the game for awhile and have had some fun. I am so thankful to everyone who has been on the game and have been so happy with it. I am so happy to be able to get back to work. I am so happy to be able to talk to everyone who has been on the game for a while and I am so happy to have a chance to meet people who are doing something awesome.\n",
            "\n",
            "\n",
            "I'm looking forward to hearing about new projects, what are your thoughts on the game and\n"
          ]
        }
      ]
    }
  ]
}